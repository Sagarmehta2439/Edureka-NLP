{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Write a program to input three sentences from user and creates the corpus\n",
    "Example:\n",
    "Let’s say these 3 are your strings:\n",
    "S1=” India won the match”\n",
    "S2=” England won the cricket match” S3=” Australia won the final match”\n",
    "Then Corpus (list of union of all words from all strings) is:\n",
    "[India, England, Australia, won, the, match, cricket, final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = \"India won the match\"\n",
    "S2 = \"England won the cricket match\"\n",
    "S3 = \"Australia won the final match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'won', 'the', 'match']\n",
      "['England', 'won', 'the', 'cricket', 'match']\n",
      "['Australia', 'won', 'the', 'final', 'match']\n"
     ]
    }
   ],
   "source": [
    "# tokenize the strings\n",
    "S1_tokens = word_tokenize(S1)\n",
    "S2_tokens = word_tokenize(S2)\n",
    "S3_tokens = word_tokenize(S3)\n",
    "print(S1_tokens)\n",
    "print(S2_tokens)\n",
    "print(S3_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Australia', 'England', 'India', 'cricket', 'final', 'match', 'the', 'won'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_corpus = set(S1_tokens).union(set(S2_tokens).union(set(S3_tokens)))\n",
    "S_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function named “MakeCorpus” which will take list of string as an input and will return a list having union of all words. Save this function in a python file named “Corpus”. This can be used for future applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeCorpus(S1,S2,S3):\n",
    "    import nltk\n",
    "    from nltk import word_tokenize\n",
    "    S1_tokens = word_tokenize(S1)\n",
    "    S2_tokens = word_tokenize(S2)\n",
    "    S3_tokens = word_tokenize(S3)\n",
    "    S_corpus = set(S1_tokens).union(set(S2_tokens).union(set(S3_tokens)))\n",
    "    return S_corpus\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = MakeCorpus(S1,S2,S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Australia', 'England', 'India', 'cricket', 'final', 'match', 'the', 'won'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Write a program to input three sentences from user and convert them into vectors. Use presence and absence of words to build the vectors.\n",
    "Example:\n",
    "Let’s say these 3 are your strings:\n",
    "S1=” India won the match”\n",
    "S2=” England won the cricket match” S3=” Australia won the final match”\n",
    "Then Corpus (list of union of all words from all strings) is:\n",
    "[India, England, Australia, won, the, match, cricket, final]\n",
    "So, S1 will be [1,0,0,1,1,1,0,0] S2 will be [0,1,0,1,1,1,1,0] S3 will be [0,0,1,1,1,1,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cricket': 0,\n",
       " 'Australia': 0,\n",
       " 'final': 0,\n",
       " 'match': 0,\n",
       " 'won': 0,\n",
       " 'India': 0,\n",
       " 'the': 0,\n",
       " 'England': 0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_corpus_dict = dict.fromkeys(S_corpus,0)\n",
    "S_corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1_dict = dict.fromkeys(S_corpus_dict,0)\n",
    "S2_dict = dict.fromkeys(S_corpus_dict,0)\n",
    "S3_dict = dict.fromkeys(S_corpus_dict,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in S1_tokens:\n",
    "    S1_dict[token]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in S2_tokens:\n",
    "    S2_dict[token]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in S3_tokens:\n",
    "    S3_dict[token]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'C': 0, 'lives': 0, 'D': 0, 'B': 0, 'with': 0, 'plays': 0}\n",
      "{'A': 0, 'C': 0, 'lives': 0, 'D': 0, 'B': 0, 'with': 0, 'plays': 0}\n",
      "{'A': 0, 'C': 0, 'lives': 0, 'D': 0, 'B': 0, 'with': 0, 'plays': 0}\n",
      "[0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(S1_dict)\n",
    "print(S2_dict)\n",
    "print(S3_dict)\n",
    "s1_list = list(S1_dict.values())\n",
    "print(s1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function named “PresenceAbsenceVectorization” which will take list of string as an input and will return a list of vectors. Save this function in a python file named “Vectorization”. This can be used for future applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PresenceAbsenceVectorization(S1,S2,S3):\n",
    "    from nltk import word_tokenize\n",
    "    S1_tokens = word_tokenize(S1)\n",
    "    S2_tokens = word_tokenize(S2)\n",
    "    S3_tokens = word_tokenize(S3)\n",
    "    S_corpus = set(S1_tokens).union(set(S2_tokens).union(set(S3_tokens)))\n",
    "    S_corpus_dict = dict.fromkeys(S_corpus,0)\n",
    "    S1_dict = dict.fromkeys(S_corpus_dict,0)\n",
    "    S2_dict = dict.fromkeys(S_corpus_dict,0)\n",
    "    S3_dict = dict.fromkeys(S_corpus_dict,0)\n",
    "    for token in S1_tokens:\n",
    "        S1_dict[token]+=1\n",
    "    for token in S2_tokens:\n",
    "        S2_dict[token]+=1\n",
    "    for token in S3_tokens:\n",
    "        S3_dict[token]+=1\n",
    "    return (S1_dict,S2_dict,S3_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cricket': 0,\n",
       "  'Australia': 0,\n",
       "  'final': 0,\n",
       "  'match': 1,\n",
       "  'won': 1,\n",
       "  'India': 1,\n",
       "  'the': 1,\n",
       "  'England': 0},\n",
       " {'cricket': 1,\n",
       "  'Australia': 0,\n",
       "  'final': 0,\n",
       "  'match': 1,\n",
       "  'won': 1,\n",
       "  'India': 0,\n",
       "  'the': 1,\n",
       "  'England': 1},\n",
       " {'cricket': 0,\n",
       "  'Australia': 1,\n",
       "  'final': 1,\n",
       "  'match': 1,\n",
       "  'won': 1,\n",
       "  'India': 0,\n",
       "  'the': 1,\n",
       "  'England': 0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PresenceAbsenceVectorization(S1,S2,S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a program to enter 3 strings from a user and vectorise them on basis of their counts.\n",
    "Example:\n",
    "Let’s say these 3 are your strings:\n",
    "S1=” A lives with B. A plays with C. “ S2=” B lives with C. B plays with D” S3=” C lives with D. C plays with A”\n",
    "Then Corpus (list of union of all words from all strings) is: [A, B, C, D, lives, with, plays]\n",
    "So, S1 will be [2,1,1,0,1,2,1] S2 will be [0,2,1,1,1,2,1] S3 will be [1,0,2,1,1,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = \"A lives with B. A plays with C.\"\n",
    "S2 = \"B lives with C. B plays with D\"\n",
    "S3 = \"C lives with D. C plays with A\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A lives with B A plays with C'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1 = S1.replace(\".\",\"\")\n",
    "S2 = S2.replace(\".\",\"\")\n",
    "S3 = S3.replace(\".\",\"\")\n",
    "S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A', 'B', 'C', 'D', 'lives', 'plays', 'with'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = MakeCorpus(S1,S2,S3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0, 'C': 0, 'lives': 0, 'D': 0, 'B': 0, 'with': 0, 'plays': 0}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountVectorization(S1,S2,S3):\n",
    "    from nltk import word_tokenize\n",
    "    S1_tokens = word_tokenize(S1)\n",
    "    S2_tokens = word_tokenize(S2)\n",
    "    S3_tokens = word_tokenize(S3)\n",
    "    S_corpus = set(S1_tokens).union(set(S2_tokens).union(set(S3_tokens)))\n",
    "    S_corpus_dict = dict.fromkeys(S_corpus,0)\n",
    "    S1_dict = dict.fromkeys(S_corpus_dict,0)\n",
    "    S2_dict = dict.fromkeys(S_corpus_dict,0)\n",
    "    S3_dict = dict.fromkeys(S_corpus_dict,0)\n",
    "    for token in S1_tokens:\n",
    "        S1_dict[token]+=1\n",
    "    for token in S2_tokens:\n",
    "        S2_dict[token]+=1\n",
    "    for token in S3_tokens:\n",
    "        S3_dict[token]+=1\n",
    "    return (S1_dict,S2_dict,S3_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'A': 2, 'C': 1, 'lives': 1, 'D': 0, 'B': 1, 'with': 2, 'plays': 1},\n",
       " {'A': 0, 'C': 1, 'lives': 1, 'D': 1, 'B': 2, 'with': 2, 'plays': 1},\n",
       " {'A': 1, 'C': 2, 'lives': 1, 'D': 1, 'B': 0, 'with': 2, 'plays': 1})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorization(S1,S2,S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write a program to input 3 strings but vectorise them using TF-IDF (Term Frequency and Inverse Document Frequency) and print the strings along with the vectors.\n",
    "You can use already available python TF-IDF Vectorizer\n",
    "(Refer : http://scikit- learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer. html )\n",
    "Create a function named “TFIDFVectorization” which will take list of string as an input and will return a list of vectors. Save this function in a python file named “Vectorization”. This can be used for future application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDFVectorization(S1,S2,S3):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tf_vect = TfidfVectorizer(min_df = 1,lowercase = True,stop_words = 'english')\n",
    "    tf_matrix = tf_vect.fit_transform([S1,S2,S3])\n",
    "    tf_df = pd.DataFrame(tf_matrix.toarray(),columns = tf_vect.get_feature_names())\n",
    "    return (tf_df,tf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = \"India won the match\"\n",
    "S2 = \"England won the cricket match\"\n",
    "S3 = \"Australia won the final match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   australia   cricket   england     final     india     match       won\n",
       " 0   0.000000  0.000000  0.000000  0.000000  0.767495  0.453295  0.453295\n",
       " 1   0.000000  0.608845  0.608845  0.000000  0.000000  0.359594  0.359594\n",
       " 2   0.608845  0.000000  0.000000  0.608845  0.000000  0.359594  0.359594,\n",
       " array([[0.        , 0.        , 0.        , 0.        , 0.76749457,\n",
       "         0.45329466, 0.45329466],\n",
       "        [0.        , 0.6088451 , 0.6088451 , 0.        , 0.        ,\n",
       "         0.35959372, 0.35959372],\n",
       "        [0.6088451 , 0.        , 0.        , 0.6088451 , 0.        ,\n",
       "         0.35959372, 0.35959372]]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDFVectorization(S1,S2,S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
